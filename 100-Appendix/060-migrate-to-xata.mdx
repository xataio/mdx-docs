---
title: Migrating from Xata Lite to Xata
navTitle: Migrating from Xata Lite to Xata
keywords: ['Xata Lite', 'Xata', 'export']
description: How to migrate data from a Xata Lite database to a Xata database.
slug: migrate-from-xata-lite-to-xata
published: true
---

<Alert status="warning">
  Unfortunately, Xata does not support file storage. If your database contains file columns, please follow one of the
  guides below for exporting your files from Xata Lite. Once exported, please drop any file columns in Xata Lite and
  proceed with the export/import guides below.
</Alert>

# Exporting a pg-enabled database from Xata Lite

1. Export the database using the pg_dump command below. See the Xata Lite [documentation](https://lite.xata.io/docs/postgres#connection-string) for guidance on finding your connection details.

```bash
pg_dump \
  --no-acl \
  --no-owner \
  --no-table-access-method \
  --no-privileges \
  --schema=public \
  --host=<region>.sql.xata.sh \
  --username=<workspace-id> \
  --dbname=<database-name>:<branch> \
  --password \
  --format=tar \
  > database_export.tar
```

Alternatively, with the connection string:

```sql
pg_dump \
   --no-acl \
   --no-owner \
   --no-table-access-method  \
   --no-privileges \
   --schema=public \
   --format=tar
   'postgresql://<workspace_id>:$XATA_API_KEY@<region>.sql.xata.sh/<dbname>:main?sslmode=require' > database_export.tar
```

# Importing a pg-enabled Xata Lite database into Xata

1. Create a main branch in your Xata project. See our [quick start guide](https://xata.io/documentation/quickstart) for further guidance.
2. Once your main branch is up and running, click the queries tab in the console and run the following SQL query. These queries create the Xata Lite objects required to maintain the xata_id column.

```sql
CREATE SCHEMA xata_private;

--
-- Name: xid; Type: DOMAIN; Schema: xata_private; Owner: -
--

CREATE DOMAIN xata_private.xid AS character(20)
	CONSTRAINT xid_check CHECK ((VALUE ~ '^[a-v0-9]{20}$'::text));

CREATE FUNCTION xata_private.xid(_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP) RETURNS xata_private.xid
    LANGUAGE plpgsql SECURITY DEFINER
    AS $$
DECLARE
    _t INT;
    _m INT;
    _p INT;
    _c INT;
BEGIN
    _t := floor(EXTRACT(epoch FROM _at));
    _m := xata_private._xid_machine_id();
    _p := pg_backend_pid();
    _c := nextval('xata_private.xid_serial')::INT;

    return xata_private.xid_encode(ARRAY [
            (_t >> 24) & 255, (_t >> 16) & 255, (_t >> 8) & 255 , _t & 255,
            (_m >> 16) & 255, (_m >> 8) & 255 , _m & 255,
            (_p >> 8) & 255, _p & 255,
            (_c >> 16) & 255, (_c >> 8) & 255 , _c & 255
        ]);
END;
$$;

CREATE FUNCTION xata_private._xid_machine_id() RETURNS integer
    LANGUAGE plpgsql IMMUTABLE
    AS $$
BEGIN
    RETURN (SELECT system_identifier & 16777215 FROM pg_control_system());
END;
$$;

CREATE FUNCTION xata_private.xid_encode(_id integer[]) RETURNS xata_private.xid
    LANGUAGE plpgsql
    AS $$
DECLARE
    _encoding CHAR(1)[] = '{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v}';
BEGIN
    RETURN _encoding[1 + (_id[1] >> 3)]
               || _encoding[1 + ((_id[2] >> 6) & 31 | (_id[1] << 2) & 31)]
               || _encoding[1 + ((_id[2] >> 1) & 31)]
               || _encoding[1 + ((_id[3] >> 4) & 31 | (_id[2] << 4) & 31)]
               || _encoding[1 + (_id[4] >> 7 | (_id[3] << 1) & 31)]
               || _encoding[1 + ((_id[4] >> 2) & 31)]
               || _encoding[1 + (_id[5] >> 5 | (_id[4] << 3) & 31)]
               || _encoding[1 + (_id[5] & 31)]
               || _encoding[1 + (_id[6] >> 3)]
               || _encoding[1 + ((_id[7] >> 6) & 31 | (_id[6] << 2) & 31)]
               || _encoding[1 + ((_id[7] >> 1) & 31)]
               || _encoding[1 + ((_id[8] >> 4) & 31 | (_id[7] << 4) & 31)]
               || _encoding[1 + (_id[9] >> 7 | (_id[8] << 1) & 31)]
               || _encoding[1 + ((_id[9] >> 2) & 31)]
               || _encoding[1 + ((_id[10] >> 5) | (_id[9] << 3) & 31)]
               || _encoding[1 + (_id[10] & 31)]
               || _encoding[1 + (_id[11] >> 3)]
               || _encoding[1 + ((_id[12] >> 6) & 31 | (_id[11] << 2) & 31)]
               || _encoding[1 + ((_id[12] >> 1) & 31)]
        || _encoding[1 + ((_id[12] << 4) & 31)];
END;
$$;

CREATE SEQUENCE xata_private.xid_serial
    START WITH 0
    INCREMENT BY 1
    MINVALUE 0
    MAXVALUE 16777215
    CACHE 1
    CYCLE;
```

3. Import the schema into Xata using pg_restore.

```bash
pg_restore --host=<xata-host> --username=xata --dbname=xata --password data_export.tar
```

Alternatively:

```sql
pg_restore --dbname=$CONN_STRING database_export.tar
```

# Export a non-pg-enabled database from Xata Lite

<Alert status="warning">
  Non-pg-enabled Xata Lite databases do not support direct connection and thus cannot be exported and imported using
  pg_dump and pg_restore respectively. Individual tables can be exported as `.csv` files via the Xata Lite console and
  imported into Xata Lite using `psql` the `COPY` SQL command.
</Alert>

1. Export tables individually to `.csv` files via the Xata Lite console.

# Import a non-pg-enabled database to Xata

1. Create a main branch in your Xata project. See our [quick start guide](https://xata.io/documentation/quickstart) for further guidance.
2. Once your main branch is up and running, click the queries tab in the console and run the following SQL query. These queries create the Xata Lite objects required to maintain the xata_id column.

```sql
CREATE SCHEMA xata_private;

--
-- Name: xid; Type: DOMAIN; Schema: xata_private; Owner: -
--

CREATE DOMAIN xata_private.xid AS character(20)
	CONSTRAINT xid_check CHECK ((VALUE ~ '^[a-v0-9]{20}$'::text));

CREATE FUNCTION xata_private.xid(_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP) RETURNS xata_private.xid
    LANGUAGE plpgsql SECURITY DEFINER
    AS $$
DECLARE
    _t INT;
    _m INT;
    _p INT;
    _c INT;
BEGIN
    _t := floor(EXTRACT(epoch FROM _at));
    _m := xata_private._xid_machine_id();
    _p := pg_backend_pid();
    _c := nextval('xata_private.xid_serial')::INT;

    return xata_private.xid_encode(ARRAY [
            (_t >> 24) & 255, (_t >> 16) & 255, (_t >> 8) & 255 , _t & 255,
            (_m >> 16) & 255, (_m >> 8) & 255 , _m & 255,
            (_p >> 8) & 255, _p & 255,
            (_c >> 16) & 255, (_c >> 8) & 255 , _c & 255
        ]);
END;
$$;

CREATE FUNCTION xata_private._xid_machine_id() RETURNS integer
    LANGUAGE plpgsql IMMUTABLE
    AS $$
BEGIN
    RETURN (SELECT system_identifier & 16777215 FROM pg_control_system());
END;
$$;

CREATE FUNCTION xata_private.xid_encode(_id integer[]) RETURNS xata_private.xid
    LANGUAGE plpgsql
    AS $$
DECLARE
    _encoding CHAR(1)[] = '{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v}';
BEGIN
    RETURN _encoding[1 + (_id[1] >> 3)]
               || _encoding[1 + ((_id[2] >> 6) & 31 | (_id[1] << 2) & 31)]
               || _encoding[1 + ((_id[2] >> 1) & 31)]
               || _encoding[1 + ((_id[3] >> 4) & 31 | (_id[2] << 4) & 31)]
               || _encoding[1 + (_id[4] >> 7 | (_id[3] << 1) & 31)]
               || _encoding[1 + ((_id[4] >> 2) & 31)]
               || _encoding[1 + (_id[5] >> 5 | (_id[4] << 3) & 31)]
               || _encoding[1 + (_id[5] & 31)]
               || _encoding[1 + (_id[6] >> 3)]
               || _encoding[1 + ((_id[7] >> 6) & 31 | (_id[6] << 2) & 31)]
               || _encoding[1 + ((_id[7] >> 1) & 31)]
               || _encoding[1 + ((_id[8] >> 4) & 31 | (_id[7] << 4) & 31)]
               || _encoding[1 + (_id[9] >> 7 | (_id[8] << 1) & 31)]
               || _encoding[1 + ((_id[9] >> 2) & 31)]
               || _encoding[1 + ((_id[10] >> 5) | (_id[9] << 3) & 31)]
               || _encoding[1 + (_id[10] & 31)]
               || _encoding[1 + (_id[11] >> 3)]
               || _encoding[1 + ((_id[12] >> 6) & 31 | (_id[11] << 2) & 31)]
               || _encoding[1 + ((_id[12] >> 1) & 31)]
        || _encoding[1 + ((_id[12] << 4) & 31)];
END;
$$;

CREATE SEQUENCE xata_private.xid_serial
    START WITH 0
    INCREMENT BY 1
    MINVALUE 0
    MAXVALUE 16777215
    CACHE 1
    CYCLE;
```

3. Create each of your tables as they were in Xata Lite. You can use the query workspace in the UI to execute the `CREATE TABLE` statements.
4. Once your schema has been created, connect to your database via psql and import each `.csv` file using the following command. To avoid foreign key constraint errors, start with linked tables, then import the tables with foreign keys.

```sql
\copy tablename FROM '/path/to/table.csv' WITH (FORMAT csv, HEADER true);
```

# Exporting files from a pg-enabled database

The following guide uses the Xata SDK to download all files in your Xata Lite database.

1. Export the schema only from your pg-enabled Xata Lite database and save it in your project directory.

```bash
pg_dump \
  --no-acl \
  --no-owner \
  --no-table-access-method \
  --no-privileges \
  --schema=public \
  --host=<host-region> \
  --username=<workspace-id> \
  --dbname=<database-name>:<branch> \
  --password \
  --schema-only \
  > xata-schema.sql
```

2. Create and execute the following script in your project directory. You can use the `--expose-gc` to help downloading larger numbers of files from multiple tables. If the script is interrupted, you can delete the subdirectory of the table that was last being downloaded. When you resume the script, tables that have already had their files exported will be skipped.

```jsx
// download-files-from-schema.mjs
import { XataClient } from './src/xata.js';
import 'dotenv/config';
import fs from 'fs';
import path from 'path';
import fetch from 'node-fetch';
import pLimit from 'p-limit';

const API_KEY = process.env.XATA_API_KEY;
const DATABASE_URL = process.env.XATA_DATABASE_URL;
const BRANCH = process.env.XATA_BRANCH || 'main';
const SCHEMA_FILE = process.env.XATA_SCHEMA_FILE || './xata-schema.sql';

const xata = new XataClient({ apiKey: API_KEY, branch: BRANCH });

if (!API_KEY || !DATABASE_URL) {
  console.error('‚ùå Missing XATA_API_KEY or XATA_DATABASE_URL in .env');
  process.exit(1);
}

if (!fs.existsSync(SCHEMA_FILE)) {
  console.error(`‚ùå Schema file not found: ${SCHEMA_FILE}`);
  process.exit(1);
}

/**
 * Parse SQL schema file to detect tables with Xata file columns.
 */
function parseSchemaForFileTables(schemaFile) {
  console.log(`üß© Reading schema from "${schemaFile}"...`);
  const sql = fs.readFileSync(schemaFile, 'utf-8');

  // Match CREATE TABLE statements
  const tableRegex = /CREATE TABLE\s+(?:public\.)?"?([\w-]+)"?\s*\(([\s\S]*?)\);/g;
  const result = {};

  let match;
  while ((match = tableRegex.exec(sql)) !== null) {
    const tableName = match[1];
    const tableBody = match[2];
    const fileColumns = [];

    // Split table body by lines to make it easier to parse
    const lines = tableBody.split('\n');

    for (const rawLine of lines) {
      const line = rawLine.trim();

      // Skip constraints or empty lines
      if (!line || line.toLowerCase().startsWith('constraint')) continue;

      // Match something like: colname xata.xata_file_array or "colname" xata.xata_file
      const colMatch = line.match(/^"?(.*?)"?\s+([a-zA-Z0-9\._]+)/);
      if (!colMatch) continue;

      const colName = colMatch[1].trim();
      const colType = colMatch[2].trim().toLowerCase();

      if (colType.includes('xata.xata_file') || colType.includes('xata_file')) {
        fileColumns.push(colName);
      }
    }

    if (fileColumns.length > 0) {
      result[tableName] = fileColumns;
    }
  }

  const tableCount = Object.keys(result).length;
  console.log(`üìä Found ${tableCount} tables with file columns.`);
  for (const [table, cols] of Object.entries(result)) {
    console.log(`  - ${table}: ${cols.join(', ')}`);
  }

  return result;
}

/**
 * Download all files from a table. Skip tables that already have download directories.
 */
async function downloadFilesFromTable(tableName, fileColumns, concurrency = 5) {
  console.log(`\nüì¶ Fetching records from "${tableName}"...`);

  if (!fileColumns.length) {
    console.log(`‚ÑπÔ∏è No file columns specified. Skipping.`);
    return;
  }

  const table = xata.db[tableName];
  if (!table) {
    console.warn(`‚ö†Ô∏è Table "${tableName}" not found. Skipping.`);
    return;
  }

  const baseDir = path.resolve(`./file-downloads/${tableName}`);
  fs.mkdirSync(baseDir, { recursive: true });

  const existingFiles = fs.readdirSync(baseDir).filter((f) => f !== '.DS_Store');
  if (existingFiles.length > 0) {
    console.log(
      `‚è≠ Table "${tableName}" already has downloaded files (${existingFiles.length}). Skipping entire table.`
    );
    return;
  }

  const columns = fileColumns.flatMap((col) => [`${col}.name`, `${col}.signedUrl`]);

  let fileCount = 0;
  let skippedCount = 0;
  let failedCount = 0;
  const limit = pLimit(concurrency);

  /** Retry wrapper */
  async function retry(fn, label, retries = 2) {
    try {
      return await fn();
    } catch (err) {
      if (retries <= 0) throw err;
      console.warn(`‚ö†Ô∏è Retry for ${label}, attempts left: ${retries}`);
      await new Promise((r) => setTimeout(r, 500));
      return retry(fn, label, retries - 1);
    }
  }

  /** Download a single file safely */
  async function downloadFile(file, recordId, colName) {
    if (!file?.name) return false;

    const ext = path.extname(file.name);
    const base = path.basename(file.name, ext);
    const newName = `${base}__${recordId}${ext}`;
    const filePath = path.join(baseDir, newName);

    if (fs.existsSync(filePath)) {
      console.log(`‚è≠ Already exists, skipping: ${newName}`);
      skippedCount++;
      return true;
    }

    const label = `${newName} (record ${recordId})`;

    // Try signedUrl from record
    if (file.signedUrl) {
      try {
        const res = await retry(() => fetch(file.signedUrl), label);
        if (res.ok && res.body) {
          await new Promise((resolve, reject) => {
            const stream = fs.createWriteStream(filePath);
            res.body.pipe(stream);
            res.body.on('error', reject);
            stream.on('finish', resolve);
          });
          console.log(`‚úÖ Saved (signedUrl): ${label}`);
          fileCount++;
          return true;
        }
      } catch (err) {
        console.warn(`‚ö†Ô∏è SignedUrl fetch failed for ${label}: ${err.message}`);
      }
    }

    // Fallback: signedUrl fetch via REST/curl if signedUrl is null in record
    if (!file.signedUrl) {
      try {
        const res = await fetch(`${DATABASE_URL}/tables/${tableName}/data/${recordId}?columns=${colName}.signedUrl`, {
          headers: { Authorization: `Bearer ${API_KEY}` }
        });
        if (res.ok) {
          const json = await res.json();
          const fetchedFile = Array.isArray(json[colName]) ? json[colName][0] : json[colName];
          if (fetchedFile?.signedUrl) {
            const signedRes = await retry(() => fetch(fetchedFile.signedUrl), label);
            if (signedRes.ok && signedRes.body) {
              await new Promise((resolve, reject) => {
                const stream = fs.createWriteStream(filePath);
                signedRes.body.pipe(stream);
                signedRes.body.on('error', reject);
                stream.on('finish', resolve);
              });
              console.log(`‚úÖ Saved (fetched signedUrl): ${label}`);
              fileCount++;
              return true;
            }
          }
        }
      } catch (err) {
        console.warn(`‚ö†Ô∏è REST signedUrl fetch failed for ${label}: ${err.message}`);
      }
    }

    console.warn(`‚ùå Could NOT download ${label}`);
    failedCount++;
    return false;
  }

  // Pagination loop
  let page = await table.select(columns).getPaginated({ pagination: { size: 20 }, consistency: 'eventual' });

  while (page && page.records.length > 0) {
    console.log(`üìÑ Processing ${page.records.length} records...`);

    for (const record of page.records) {
      for (const col of fileColumns) {
        const value = record[col];
        if (!value) continue;
        const files = Array.isArray(value) ? value : [value];

        const tasks = files.map((file) => limit(() => downloadFile(file, record.xata_id, col)));
        await Promise.all(tasks);
      }
    }

    const more = page.meta?.page?.more;
    if (!more) break;
    page = await page.nextPage();
  }

  console.log(`\nüéâ Finished downloading files for "${tableName}"`);
  console.log(`   ‚úî Successfully downloaded: ${fileCount}`);
  console.log(`   ‚è≠ Skipped (already exists): ${skippedCount}`);
  console.log(`   ‚ùå Failed:                 ${failedCount}`);
}

/**
 * Main entry
 */
async function main() {
  try {
    const tablesWithFiles = parseSchemaForFileTables(SCHEMA_FILE);

    if (Object.keys(tablesWithFiles).length === 0) {
      console.log('‚ÑπÔ∏è No tables with file columns found. Exiting.');
      return;
    }

    for (const [tableName, fileColumns] of Object.entries(tablesWithFiles)) {
      try {
        await downloadFilesFromTable(tableName, fileColumns);
      } catch (err) {
        console.error(`‚ùå Error processing table "${tableName}":`, err.message);
      }
    }

    console.log('\n‚úÖ All downloads complete.');
  } catch (err) {
    console.error('‚ùå Fatal error:', err.message);
  }
}

await main();
```

# Exporting files from a non-pg-enabled database

The following script uses the Xata SDK to download all files in your Xata Lite database. You can use the `--expose-gc` to help downloading larger numbers of files from multiple tables. If the script is interrupted, you can delete the subdirectory of the table that was last being downloaded. When you resume the script, tables that have already had their files exported will be skipped.

```jsx
import { getXataClient } from './src/xata.js';
import 'dotenv/config';
import fs from 'fs';
import path from 'path';
import fetch from 'node-fetch';
import pLimit from 'p-limit';

const xata = getXataClient();

const API_KEY = process.env.XATA_API_KEY;
const DATABASE_URL = process.env.XATA_DATABASE_URL;
const BRANCH = process.env.XATA_BRANCH || 'main';

if (!API_KEY || !DATABASE_URL) {
  console.error('‚ùå Missing XATA_API_KEY or XATA_DATABASE_URL in .env');
  process.exit(1);
}

/**
 * Fetch all tables and detect which ones contain file-type columns.
 */
async function getAllTablesWithFiles() {
  console.log('üì° Fetching full database schema...');

  const baseUrl = DATABASE_URL.includes(':') ? DATABASE_URL : `${DATABASE_URL}:${BRANCH}`;

  const res = await fetch(baseUrl, {
    headers: {
      Authorization: `Bearer ${API_KEY}`,
      Accept: 'application/json'
    }
  });

  if (!res.ok) {
    throw new Error(`‚ùå Failed to fetch database schema: ${res.status} ${res.statusText} (${baseUrl})`);
  }

  const data = await res.json();

  if (!data?.schema?.tables) {
    throw new Error('‚ùå Schema format not recognized ‚Äî missing "schema.tables"');
  }

  const result = {};
  for (const table of data.schema.tables) {
    const fileColumns = (table.columns || [])
      .filter((col) => col.type === 'file' || col.type === 'file[]')
      .map((col) => col.name);

    if (fileColumns.length > 0) {
      result[table.name] = fileColumns;
    }
  }

  const tableCount = Object.keys(result).length;
  console.log(`üìä Found ${tableCount} tables with file columns.`);
  for (const [table, cols] of Object.entries(result)) {
    console.log(`  - ${table}: ${cols.join(', ')}`);
  }

  return result;
}

async function downloadFilesFromTable(tableName, fileColumns, concurrency = 5) {
  console.log(`\nüì¶ Fetching records from "${tableName}"...`);

  if (!fileColumns.length) {
    console.log(`‚ÑπÔ∏è No file columns specified. Skipping.`);
    return;
  }

  const table = xata.db[tableName];
  if (!table) {
    console.warn(`‚ö†Ô∏è Table "${tableName}" not found. Skipping.`);
    return;
  }

  const baseDir = path.resolve(`./file-downloads/${tableName}`);
  fs.mkdirSync(baseDir, { recursive: true });

  const existingFiles = fs.readdirSync(baseDir).filter((f) => f !== '.DS_Store');
  if (existingFiles.length > 0) {
    console.log(
      `‚è≠ Table "${tableName}" already has downloaded files (${existingFiles.length}). Skipping entire table.`
    );
    return;
  }

  const columns = fileColumns.flatMap((col) => [`${col}.name`, `${col}.signedUrl`]);

  let fileCount = 0;
  let skippedCount = 0;
  let failedCount = 0;
  const limit = pLimit(concurrency);

  /** Retry wrapper */
  async function retry(fn, label, retries = 2) {
    try {
      return await fn();
    } catch (err) {
      if (retries <= 0) throw err;
      console.warn(`‚ö†Ô∏è Retry for ${label}, attempts left: ${retries}`);
      await new Promise((r) => setTimeout(r, 500));
      return retry(fn, label, retries - 1);
    }
  }

  /** Download a single file safely */
  async function downloadFile(file, recordId, colName) {
    if (!file?.name) return false;

    const ext = path.extname(file.name);
    const base = path.basename(file.name, ext);
    const newName = `${base}__${recordId}${ext}`;
    const filePath = path.join(baseDir, newName);

    if (fs.existsSync(filePath)) {
      console.log(`‚è≠ Already exists, skipping: ${newName}`);
      skippedCount++;
      return true;
    }

    const label = `${newName} (record ${recordId})`;

    // Try signedUrl from record
    if (file.signedUrl) {
      try {
        const res = await retry(() => fetch(file.signedUrl), label);
        if (res.ok && res.body) {
          await new Promise((resolve, reject) => {
            const stream = fs.createWriteStream(filePath);
            res.body.pipe(stream);
            res.body.on('error', reject);
            stream.on('finish', resolve);
          });
          console.log(`‚úÖ Saved (signedUrl): ${label}`);
          fileCount++;
          return true;
        }
      } catch (err) {
        console.warn(`‚ö†Ô∏è SignedUrl fetch failed for ${label}: ${err.message}`);
      }
    }

    // Fallback: signedUrl fetch via REST/curl if signedUrl is null in record
    if (!file.signedUrl) {
      try {
        const res = await fetch(`${DATABASE_URL}/tables/${tableName}/data/${recordId}?columns=${colName}.signedUrl`, {
          headers: { Authorization: `Bearer ${API_KEY}` }
        });
        if (res.ok) {
          const json = await res.json();
          const fetchedFile = Array.isArray(json[colName]) ? json[colName][0] : json[colName];
          if (fetchedFile?.signedUrl) {
            const signedRes = await retry(() => fetch(fetchedFile.signedUrl), label);
            if (signedRes.ok && signedRes.body) {
              await new Promise((resolve, reject) => {
                const stream = fs.createWriteStream(filePath);
                signedRes.body.pipe(stream);
                signedRes.body.on('error', reject);
                stream.on('finish', resolve);
              });
              console.log(`‚úÖ Saved (fetched signedUrl): ${label}`);
              fileCount++;
              return true;
            }
          }
        }
      } catch (err) {
        console.warn(`‚ö†Ô∏è REST signedUrl fetch failed for ${label}: ${err.message}`);
      }
    }

    console.warn(`‚ùå Could NOT download ${label}`);
    failedCount++;
    return false;
  }

  // Pagination loop
  let page = await table.select(columns).getPaginated({ pagination: { size: 20 }, consistency: 'eventual' });

  while (page && page.records.length > 0) {
    console.log(`üìÑ Processing ${page.records.length} records...`);

    for (const record of page.records) {
      for (const col of fileColumns) {
        const value = record[col];
        if (!value) continue;
        const files = Array.isArray(value) ? value : [value];

        const tasks = files.map((file) => limit(() => downloadFile(file, record.id, col)));
        await Promise.all(tasks);
      }
    }

    const more = page.meta?.page?.more;
    if (!more) break;
    page = await page.nextPage();
  }

  console.log(`\nüéâ Finished downloading files for "${tableName}"`);
  console.log(`   ‚úî Successfully downloaded: ${fileCount}`);
  console.log(`   ‚è≠ Skipped (already exists): ${skippedCount}`);
  console.log(`   ‚ùå Failed:                 ${failedCount}`);
}

/**
 * Main function: automatically detects all tables with file columns and downloads all files efficiently
 */
async function main() {
  try {
    const tablesWithFiles = await getAllTablesWithFiles();

    if (Object.keys(tablesWithFiles).length === 0) {
      console.log('‚ÑπÔ∏è No tables with file columns found. Exiting.');
      return;
    }

    for (const [tableName, fileColumns] of Object.entries(tablesWithFiles)) {
      try {
        await downloadFilesFromTable(tableName, fileColumns);
      } catch (err) {
        console.error(`‚ùå Error processing table "${tableName}":`, err.message);
      }
    }

    console.log('\n‚úÖ All downloads complete.');
  } catch (err) {
    console.error('‚ùå Fatal error:', err.message);
  }
}

await main();
```
